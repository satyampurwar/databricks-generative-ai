{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9363e693-4673-4969-acab-5c879df28c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "📘 **Introduction**  \n",
    "This Jupyter Notebook demonstrates the integration of Databricks and LangChain to build an advanced pipeline for document ingestion, text chunking, embedding, vector search, and LLM-powered question answering.\n",
    "\n",
    "🧩 **Objective**  \n",
    "The primary objective of this notebook is to extract information from a PDF document, transform the content into searchable text chunks, and enable semantic search and LLM-driven question answering using Databricks-hosted large language models.\n",
    "\n",
    "🛠️ **Problem Description**  \n",
    "With the increasing volume of unstructured data (e.g., PDFs), traditional keyword search methods are often inadequate. This notebook addresses this challenge by leveraging vector search and LLM inference, allowing users to ask natural language questions and receive context-aware answers.\n",
    "\n",
    "📂 **Datasets and Sources**  \n",
    "- **Input:** A PDF file stored in Unity Catalog Volumes  \n",
    "- **Processing:** The PDF is parsed into raw text using PyMuPDF, then chunked for embedding using LangChain's `RecursiveCharacterTextSplitter`.  \n",
    "- **Storage & Indexing:** The processed chunks are stored in a Delta Lake table and indexed using Databricks Mosaic AI Vector Search.\n",
    "\n",
    "🧰 **Libraries & Tools**  \n",
    "- `databricks-sdk`, `databricks-langchain`, `langchain`, `langchain_core`, `PyMuPDF`\n",
    "- Databricks LLMs (Meta LLaMA 3) and embedding model (`databricks-gte-large-en`)\n",
    "\n",
    "📋 **Notebook Structure**  \n",
    "1. **Environment Setup:** Install required packages and initialize services  \n",
    "2. **LLM Initialization:** Configure and test the Databricks-hosted LLaMA-3 model  \n",
    "3. **PDF Processing:** Load and extract raw text using PyMuPDF  \n",
    "4. **Text Chunking:** Split extracted text into semantically meaningful segments  \n",
    "5. **Vector Indexing:** Store and index chunks using Databricks Vector Search  \n",
    "6. **Semantic Search:** Retrieve relevant chunks based on user queries  \n",
    "7. **LLM Inference:** Use retrieved chunks to generate intelligent answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfdd3b5b-fe10-4bb5-b1e5-a41b83ba3f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries for Databricks, LangChain, MLflow, and related tools\n",
    "%pip install -U --quiet databricks-sdk==0.49.0 \"databricks-langchain>=0.4.0\" databricks-agents mlflow[databricks] databricks-vectorsearch==0.55 langchain==0.3.25 langchain_core==0.3.59 bs4==0.0.2 markdownify==0.14.1 pydantic==2.10.1 mlflow openai PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adca59d9-1e69-43c0-a225-5236050afe36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart the Python process to ensure all installed libraries are properly loaded\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "891c8637-9cf6-46e6-b0d3-da05f12b1a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# Initialize the ChatDatabricks model with specified parameters\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",  # Endpoint for the chat model\n",
    "    temperature=0.1,  # Controls the randomness of the response\n",
    "    max_tokens=250,  # Maximum number of tokens in the response\n",
    ")\n",
    "\n",
    "# Invoke the model with a query\n",
    "chat_model.invoke(\"Who is data fudiciary?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30276bff-fbe0-4406-922f-9eb9f71487a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to the PDF file to be processed\n",
    "file_path = '/Volumes/workspace/default/raw_files/dpact.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8249d4-fe69-42f7-b574-9994d0dc1345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Read the PDF file as bytes from the specified path\n",
    "with open(file_path, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "# Open the PDF using PyMuPDF from bytes\n",
    "doc = fitz.open(\"pdf\", pdf_bytes)\n",
    "\n",
    "# Extract text from all pages and concatenate into a single string\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "# Print the extracted text (for debugging or inspection)\n",
    "print(text)  # Print first 1000 characters\n",
    "\n",
    "# Example code for reading CSV, Parquet, or JSON files using Spark DataFrames:\n",
    "# df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/<catalog>/<schema>/<volume>/filename.csv\")\n",
    "# df = spark.read.parquet(\"/Volumes/<catalog>/<schema>/<volume>/filename.parquet\")\n",
    "# df = spark.read.option(\"multiline\", \"true\").json(\"/Volumes/<catalog>/<schema>/<volume>/filename.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53c25ad-2d3a-4852-a312-3fe43c04264f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Load documents (e.g., text files or PDFs from DBFS or local)\n",
    "raw_text = text  # Text extracted from the PDF in the previous cell\n",
    "\n",
    "# Chunk documents for embedding\n",
    "# Initialize the text splitter with a chunk size of 500 characters and an overlap of 100 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Create documents by splitting the raw text into chunks\n",
    "docs = text_splitter.create_documents([raw_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9175330b-ceb1-4207-9641-642eb7e5e677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the number of document chunks created\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d0024ea-8738-44bb-82b1-c47570a572c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba39c550-f400-4f1c-b3fb-a6e13c677e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas library\n",
    "\n",
    "# Convert docs to a list of dicts for display\n",
    "pd_docs = pd.DataFrame([doc.dict() for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20ca997-e664-4705-b740-8a40fb669fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add an 'id_pk' column as the first column with sequential IDs\n",
    "pd_docs.insert(0, \"id_pk\", range(1, len(pd_docs) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5067f147-40f3-4b26-b45f-4fcf4009e7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pd_docs)  # Display the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4189ed-1dbe-45f5-be3e-9bd94941d92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to a Spark DataFrame with only 'id_pk' and 'page_content' columns\n",
    "spark_df = spark.createDataFrame(pd_docs[['id_pk','page_content']])\n",
    "\n",
    "# Write the Spark DataFrame to a Delta table named 'dpact_chunks' in the 'workspace.default' schema\n",
    "# Overwrite the table if it exists and update the schema if necessary\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"workspace.default.dpact_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac4411e-1a10-47fd-82f8-97ad2d38da48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the VectorSearchClient from the databricks.vector_search.client module\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Instantiate the VectorSearchClient\n",
    "client = VectorSearchClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f798cfd6-1c44-4fac-a1b3-432410468305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable change data feed for the source table to track row-level changes\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE workspace.default.dpact_chunks\n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "# Delete the existing vector search endpoint if it exists\n",
    "client.delete_endpoint(name=\"vs_endpoint\")\n",
    "\n",
    "# Create a new vector search endpoint of type STANDARD\n",
    "client.create_endpoint(\n",
    "    name=\"vs_endpoint\",\n",
    "    endpoint_type=\"STANDARD\"\n",
    ")\n",
    "\n",
    "# Create a delta sync index for vector search:\n",
    "# - Uses 'workspace.default.dpact_chunks' as the source table\n",
    "# - Embeddings are generated from the 'page_content' column\n",
    "# - 'id_pk' is used as the primary key\n",
    "# - Index is created on the 'vs_endpoint'\n",
    "# - Embedding model endpoint: 'databricks-gte-large-en'\n",
    "# - Pipeline type is set to TRIGGERED (manual sync)\n",
    "vs_index = client.create_delta_sync_index(\n",
    "    endpoint_name=\"vs_endpoint\",\n",
    "    index_name=\"workspace.default.document_index\",\n",
    "    source_table_name=\"workspace.default.dpact_chunks\",\n",
    "    embedding_source_column=\"page_content\",\n",
    "    primary_key=\"id_pk\",\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821f9607-5acd-411c-ae0b-7a426b3b2272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all existing vector search endpoints\n",
    "client.list_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f720cd13-22c4-438f-8a04-be2b5026ad3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all indexes on the 'vs_endpoint' vector search endpoint\n",
    "client.list_indexes(\"vs_endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e97016-a930-40f0-8670-2f1441985f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the index named 'workspace.default.document_index' from the vector search endpoint\n",
    "index = client.get_index(index_name=\"workspace.default.document_index\")\n",
    "\n",
    "# Describe the retrieved index to get its metadata and details\n",
    "index.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3351ef84-472f-4aed-a577-a6efe3ed7e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform a similarity search on the vector index for the query \"Who is Data Fudiciary?\"\n",
    "# Retrieve the top 2 most similar results, selecting only the 'id_pk' and 'page_content' columns\n",
    "results_dict = index.similarity_search(\n",
    "    query_text=\"Who is Data Fudiciary?\",\n",
    "    columns=[\"id_pk\", \"page_content\"],\n",
    "    num_results=2,\n",
    ")\n",
    "\n",
    "# Display the search results in a rich tabular format\n",
    "display(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfb4606-e979-4bff-a127-85f2432ba558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the input string for the chat model by combining a prompt with the search results\n",
    "input = \"Answer Question \" + str(results_dict) + \" Question: Who is the data Fudiciary?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23810fa8-e71c-497d-bcc0-a5420490de36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the ChatDatabricks class for interacting with Databricks-hosted LLMs\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# Initialize the chat model with the specified endpoint and parameters\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",  # LLM endpoint name\n",
    "    temperature=0.1,  # Controls randomness of responses\n",
    "    max_tokens=250,   # Maximum tokens in the response\n",
    ")\n",
    "\n",
    "# Invoke the chat model with the prepared input string and return the response\n",
    "chat_model.invoke(str(input))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rag_implementation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
